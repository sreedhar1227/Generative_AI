{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9b6340a-bd95-4363-9f1d-c58a6e9f74e8",
   "metadata": {},
   "source": [
    "# Setting up OPEN AI API KEY in the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc3d20d1-2238-49c9-85f4-e0d977d1bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1fab72b-b6e6-4867-9b82-33a0bae218b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) #gpt-4o-mini\n",
    "        return llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67d4d6-0a3f-4bf4-8f06-53c1262818ac",
   "metadata": {},
   "source": [
    "# Purpose of the Code:\n",
    "This function loads and initializes a language model (LLM) using LangChain’s ChatOpenAI class, which connects to OpenAI's GPT models. It sets the specific model and behavior for generating responses.\n",
    "\n",
    "# Function Definition:\n",
    "Defines a function named load_llm that initializes and returns a pre-configured instance of a language model.\n",
    "\n",
    "# Import the Required Class:\n",
    "ChatOpenAI: A LangChain class for interacting with OpenAI’s chat-based models (like gpt-3.5-turbo and gpt-4).\n",
    "\n",
    "# Initialize the LLM:\n",
    "model_name=\"gpt-3.5-turbo\": Specifies the OpenAI model to use. In this case, it uses GPT-3.5-Turbo, a fast and cost-efficient version of OpenAI's GPT models.\n",
    "\n",
    "You can replace it with another model (e.g., \"gpt-4\" or \"gpt-4o-mini\" if supported).\n",
    "\n",
    "temperature=0: Controls the randomness of the model’s output.\n",
    "\n",
    "temperature=0: Produces deterministic and consistent responses, making it ideal for tasks requiring factual accuracy.\n",
    "\n",
    "Higher temperatures (e.g., 0.7) would make responses more creative and varied but less predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5621a4e-138e-42c6-b137-7c79269b0690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def load_prompt():\n",
    "        prompt = \"\"\" You need to answer the question in the sentence as same as in the  pdf content. . \n",
    "        Given below is the context and question of the user.\n",
    "        context = {context}\n",
    "        question = {question}\n",
    "        if the answer is not in the pdf , answer \"i donot know what the hell you are asking about\"\n",
    "         \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(prompt)\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc9641-76f8-4835-bc2c-a4f22223a8a7",
   "metadata": {},
   "source": [
    "# Purpose of the Prompt:\n",
    "This instructs the language model to:\n",
    "Provide answers strictly based on the content of the PDF.\n",
    "Use the given context (a portion of the PDF content) and the question (user input) to derive the response.\n",
    "Return \"i donot know what the hell you are asking about\" if the question cannot be answered from the provided context.\n",
    "# Dynamic Variables:\n",
    "{context}: Will be replaced with relevant text extracted from the PDF during execution.\n",
    "{question}: Will be replaced with the user’s query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd31def-aaa1-4bad-8991-4e3dc4bccd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dependencies\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def load_knowledgeBasee():\n",
    "        embeddings=OpenAIEmbeddings()\n",
    "        DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "        db = FAISS.load_local(DB_FAISS_PATH, embeddings,allow_dangerous_deserialization=True)\n",
    "        return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cc9ef5-ec4e-485b-ac4e-fb2665ad6e38",
   "metadata": {},
   "source": [
    "# Purpose of the Code:\n",
    "The function load_knowledgeBasee loads a pre-existing FAISS vector store (which contains document embeddings) from a local directory. This makes it possible to perform semantic searches or retrieve relevant content without rebuilding the vector store every time.\n",
    "# Define the Function:\n",
    "A function named load_knowledgeBasee is defined to encapsulate the process of loading a saved FAISS vector store.\n",
    "# Initialize Embeddings:\n",
    "Purpose:\n",
    "\n",
    "Creates an instance of OpenAIEmbeddings to match the embedding format of the saved FAISS vector store.\n",
    "These embeddings ensure compatibility when performing searches on the loaded vector store.\n",
    "\n",
    "Details:\n",
    "\n",
    "OpenAIEmbeddings requires an API key from OpenAI, which should already be set in the environment for this to work.\n",
    "# Define the Path to the FAISS Vector Store:\n",
    "pecifies the directory where the FAISS vector store is stored (vectorstore/db_faiss).\n",
    "# Load the FAISS Vector Store:\n",
    "FAISS.load_local:\n",
    "Loads the vector store from the local directory (DB_FAISS_PATH).\n",
    "Uses the embeddings instance for compatibility with the stored data.\n",
    "\n",
    "Parameter: allow_dangerous_deserialization=True:\n",
    "Ensures that deserialization of the stored FAISS vector store is allowed, even if there are potential risks.\n",
    "This should be used cautiously in secure environments to avoid loading untrusted or tampered files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13e4003d-41ee-4ea9-a0ca-7a96882d8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledgeBase=load_knowledgeBasee()\n",
    "llm=load_llm()\n",
    "prompt=load_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29121cdc-2ed7-44c1-a602-59faa07773d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d6bd6-8908-4768-b4e7-0f583a511f68",
   "metadata": {},
   "source": [
    "# Purpose of the Code:\n",
    "The function format_docs takes a list of document objects (docs) and extracts their text content. It combines the content into a single string, with two newlines (\\n\\n) separating the individual documents.\n",
    "\n",
    "# Extract and Combine Document Content:\n",
    "doc.page_content:\n",
    "Accesses the page_content attribute of each document object in the docs list.\n",
    "Assumes that each document object has this attribute containing its text.\n",
    "\n",
    "for doc in docs:\n",
    "Iterates over each document object in the docs list.\n",
    "\n",
    "\"\\n\\n\".join(...):\n",
    "Joins the extracted text from all documents into a single string.\n",
    "The two newline characters (\\n\\n) are used as separators between the content of different documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cc65f6e-6409-42d5-911e-d6fcd60f3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "query = \"who is krishna\"\n",
    "if True:\n",
    "    similar_embeddings=knowledgeBase.similarity_search(query)\n",
    "    similar_embeddings=FAISS.from_documents(documents=similar_embeddings, embedding=OpenAIEmbeddings())\n",
    "                \n",
    "    #creating the chain for integrating llm,prompt,stroutputparser\n",
    "    retriever = similar_embeddings.as_retriever()\n",
    "    rag_chain = (\n",
    "                        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "                        | prompt\n",
    "                        | llm\n",
    "                        | StrOutputParser()\n",
    "                    )\n",
    "                \n",
    "    response=rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8a8db45-9254-4eac-903f-0f9e12171b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Krishna is described as the reservoir of pleasure and the primeval cause of everything. He is also known as Govinda, or one who gives pleasure to the senses. Krishna establishes Himself as the source of the material and spiritual worlds, and the origin of Brahman and Paramatma. He is worshipped by those whose eyes are anointed with the salve of divine love. Krishna is the Original Person who is always meditated upon by His beloved devotees.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a2c7a4-51b7-439f-8f6c-02e1e58fbb32",
   "metadata": {},
   "source": [
    "# Purpose of the Code:\n",
    "This code performs a retrieval-augmented generation (RAG) workflow using LangChain components. It:\n",
    "\n",
    "1)Retrieves relevant documents from a knowledge base based on a query.\n",
    "2)Reformats the retrieved documents.\n",
    "3)Combines the context and query into a structured prompt.\n",
    "4)Passes the prompt to a language model (LLM) to generate a response.\n",
    "5)Parses and outputs the response.\n",
    "\n",
    "# Retrieve Similar Documents:\n",
    "knowledgeBase.similarity_search(query):\n",
    "Searches the FAISS vector store (knowledgeBase) for documents most similar to the query.\n",
    "Returns a list of document objects containing the most relevant content.\n",
    "\n",
    "# Create a Subvector Store:\n",
    "Purpose:\n",
    "Creates a new FAISS vector store containing only the retrieved similar documents.\n",
    "Uses the same OpenAIEmbeddings to ensure compatibility.\n",
    "\n",
    "Why?:\n",
    "By creating a subvector store, subsequent operations can work with only the most relevant subset of the knowledge base.\n",
    "\n",
    "# Create a Retriever:\n",
    "Converts the subvector store (similar_embeddings) into a retriever.\n",
    "A retriever is a simplified interface to search and retrieve text from a vector store.\n",
    "\n",
    "#  Define the RAG Chain:\n",
    "1)RAG Chain Workflow:\n",
    "Input as a Dictionary:\n",
    "Input consists of two keys:\n",
    "\"context\": Retrieved documents are processed through retriever and formatted by format_docs.\n",
    "\"question\": The query is passed directly through RunnablePassthrough without changes.\n",
    "\n",
    "2)prompt:\n",
    "Combines the \"context\" and \"question\" into a structured prompt (created by load_prompt()).\n",
    "\n",
    "3)llm:\n",
    "The prompt is passed to the loaded language model (llm), which generates a response.\n",
    "\n",
    "4)StrOutputParser:\n",
    "Converts the response from the LLM into a simple string format for easier use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bcd97b-20c1-407c-b805-a26ab5992066",
   "metadata": {},
   "source": [
    "# Checking the Performance by changing the model name and temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18327aae-f53b-42df-bcde-5d0cf14a39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "def load_llm():\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0.7) #gpt-4o-mini\n",
    "        return llm\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def load_prompt():\n",
    "        prompt = \"\"\" You need to answer the question in the sentence as same as in the  pdf content. . \n",
    "        Given below is the context and question of the user.\n",
    "        context = {context}\n",
    "        question = {question}\n",
    "        if the answer is not in the pdf , answer \"i donot know what the hell you are asking about\"\n",
    "         \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(prompt)\n",
    "        return prompt\n",
    "\n",
    "def load_knowledgeBasee():\n",
    "        embeddings=OpenAIEmbeddings()\n",
    "        DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "        db = FAISS.load_local(DB_FAISS_PATH, embeddings,allow_dangerous_deserialization=True)\n",
    "        return db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "106b3947-4034-42d8-8363-4ba3c78eefbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kurukṣetra is a holy place 160 km north of Delhi in the state of Haryana in India. It was the location where the great Mahābhārata war was fought between the Pāṇḍavas and the Kauravas, and where Śrī Kṛṣṇa spoke the Bhagavad-gītā to His devotee, Arjuna.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledgeBase=load_knowledgeBasee()\n",
    "llm=load_llm()\n",
    "prompt=load_prompt()\n",
    "\n",
    "def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "query = \"What is kurukshetra\"\n",
    "if True:\n",
    "    similar_embeddings=knowledgeBase.similarity_search(query)\n",
    "    similar_embeddings=FAISS.from_documents(documents=similar_embeddings, embedding=OpenAIEmbeddings())\n",
    "                \n",
    "    #creating the chain for integrating llm,prompt,stroutputparser\n",
    "    retriever = similar_embeddings.as_retriever()\n",
    "    rag_chain = (\n",
    "                        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "                        | prompt\n",
    "                        | llm\n",
    "                        | StrOutputParser()\n",
    "                    )\n",
    "                \n",
    "    response1=rag_chain.invoke(query)\n",
    "\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8096ce9-2ff3-4b7c-8206-2ec2b7eb3698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response to Query 1: Arjuna is popular for being a key warrior in the Mahabharata and for his role in the Bhagavad-gita, where he receives guidance from Lord Krishna on duty, righteousness, and self-realization during the Kurukshetra war.\n"
     ]
    }
   ],
   "source": [
    "# First Query\n",
    "query = \"FOr which Arjuna is popular for\"\n",
    "response1 = rag_chain.invoke(query)\n",
    "print(\"Response to Query 1:\", response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b5c3162-5e14-4c9a-92f4-e32972e53f9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I do not know what the hell you are asking about.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second Query\n",
    "query = \"FOr which skill Arjuna is popular for?\"\n",
    "response1 = rag_chain.invoke(query)\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2b1cfe3-d3be-44cb-8669-28f6f59ccf7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The names of the Pandavas are not mentioned in the provided content. I do not know what the hell you are asking about.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third Query\n",
    "query = \"Provide me names of Pandavas?\"\n",
    "response1 = rag_chain.invoke(query)\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b70a8-d1b2-4106-ba2f-c90cc33533b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who is dusyasana?\"\n",
    "response1 = rag_chain.invoke(query)\n",
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2666e-b213-4c5d-8294-4e6e23e2d40c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
